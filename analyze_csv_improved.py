#!/usr/bin/env python3
"""
æ”¹è¿›çš„CSVæ•°æ®åˆ†æå·¥å…·
æ•´åˆDMMé£é™©é¢„æµ‹å’Œæ–‡ä»¶å½±å“é¢„æµ‹åŠŸèƒ½
æ”¯æŒä»GitHubä»“åº“ç›´æ¥æ”¶é›†æ•°æ®
"""

import pandas as pd
import numpy as np
import os
import sys
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from core.analyzer import GitAnalyzer
from analysis.risk_predictor import RiskPredictor
from analysis.file_impact_predictor import FileImpactPredictor
from data_collection.repository_collector import RepositoryCollector
from utils.logger import setup_logger


def load_csv_data(csv_file: str) -> pd.DataFrame:
    """
    åŠ è½½CSVæ•°æ®æ–‡ä»¶
    
    Args:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        
    Returns:
        DataFrameå¯¹è±¡
    """
    try:
        # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
        
        for encoding in encodings:
            try:
                df = pd.read_csv(csv_file, encoding=encoding)
                print(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç åŠ è½½æ–‡ä»¶: {csv_file}")
                return df
            except UnicodeDecodeError:
                continue
        
        raise ValueError(f"æ— æ³•è§£ç æ–‡ä»¶ {csv_file}")
        
    except Exception as e:
        print(f"åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
        return pd.DataFrame()


def collect_repository_data(repo_url: str, 
                          output_file: str = None,
                          since: str = None,
                          to: str = None,
                          branch: str = None,
                          file_types: list = None,
                          skip_empty_dmm: bool = True,
                          resume: bool = True,
                          logger=None) -> str:
    """
    ä»GitHubä»“åº“æ”¶é›†commitæ•°æ®
    
    Args:
        repo_url: GitHubä»“åº“URL
        output_file: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        since: å¼€å§‹æ—¶é—´ (YYYY-MM-DD)
        to: ç»“æŸæ—¶é—´ (YYYY-MM-DD)
        branch: æŒ‡å®šåˆ†æ”¯
        file_types: æ–‡ä»¶ç±»å‹åˆ—è¡¨
        skip_empty_dmm: è·³è¿‡DMMå€¼ä¸ºç©ºçš„æäº¤
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        ç”Ÿæˆçš„CSVæ–‡ä»¶è·¯å¾„
    """
    if logger:
        logger.info(f"å¼€å§‹ä»GitHubä»“åº“æ”¶é›†æ•°æ®: {repo_url}")
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
    if not output_file:
        repo_name = repo_url.split('/')[-1].replace('.git', '')
        output_file = f"data/raw/commits_{repo_name}.csv"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # è§£ææ—¶é—´å‚æ•°
    since_date = None
    to_date = None
    if since:
        try:
            since_date = datetime.strptime(since, '%Y-%m-%d')
        except ValueError:
            if logger:
                logger.warning(f"æ— æ•ˆçš„å¼€å§‹æ—¶é—´æ ¼å¼: {since}")
    
    if to:
        try:
            to_date = datetime.strptime(to, '%Y-%m-%d')
        except ValueError:
            if logger:
                logger.warning(f"æ— æ•ˆçš„ç»“æŸæ—¶é—´æ ¼å¼: {to}")
    
    # åˆ›å»ºæ”¶é›†å™¨å¹¶æ”¶é›†æ•°æ®
    collector = RepositoryCollector()
    
    try:
        result_file = collector.collect_repository_data(
            repo_url=repo_url,
            output_file=output_file,
            since=since_date,
            to=to_date,
            only_in_branch=branch,
            only_modifications_with_file_types=file_types,
            skip_empty_dmm=skip_empty_dmm,
            resume=resume
        )
        
        if logger:
            logger.info(f"æ•°æ®æ”¶é›†å®Œæˆï¼Œä¿å­˜åˆ°: {result_file}")
        
        return result_file
        
    except Exception as e:
        if logger:
            logger.error(f"æ”¶é›†ä»“åº“æ•°æ®å¤±è´¥: {str(e)}")
        raise


def analyze_dmm_risk(data: pd.DataFrame, logger) -> dict:
    """åˆ†æDMMé£é™©"""
    logger.info("å¼€å§‹DMMé£é™©åˆ†æ...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰DMMå­—æ®µ
    dmm_fields = ['dmm_unit_size', 'dmm_unit_complexity', 'dmm_unit_interfacing']
    if not all(field in data.columns for field in dmm_fields):
        logger.warning("ç¼ºå°‘DMMå­—æ®µï¼Œæ— æ³•è¿›è¡Œé£é™©é¢„æµ‹")
        return {'error': 'ç¼ºå°‘DMMå­—æ®µ'}
    
    risk_predictor = RiskPredictor(logger)
    results = risk_predictor.predict_dmm_risk(data)
    
    return results


def analyze_file_impact(commit_data: pd.DataFrame, rule_file: str, target_file: str, logger) -> dict:
    """åˆ†ææ–‡ä»¶å½±å“"""
    logger.info(f"å¼€å§‹åˆ†ææ–‡ä»¶ {target_file} çš„å½±å“...")
    
    # åŠ è½½å…³è”è§„åˆ™æ•°æ®
    if not os.path.exists(rule_file):
        logger.warning(f"å…³è”è§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨: {rule_file}")
        return {'error': 'å…³è”è§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨'}
    
    rule_data = load_csv_data(rule_file)
    if rule_data.empty:
        return {'error': 'å…³è”è§„åˆ™æ•°æ®ä¸ºç©º'}
    
    impact_predictor = FileImpactPredictor(logger)
    results = impact_predictor.predict_file_impact(commit_data, rule_data, target_file)
    
    return results


def comprehensive_analysis(csv_file: str, project_name: str, output_dir: str = None):
    """
    æ‰§è¡Œç»¼åˆåˆ†æ
    
    Args:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        project_name: é¡¹ç›®åç§°
        output_dir: è¾“å‡ºç›®å½•
    """
    logger = setup_logger('ComprehensiveAnalyzer')
    
    try:
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir:
            output_path = Path(output_dir)
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = Path(f"analysis_results_{project_name}_{timestamp}")
        
        output_path.mkdir(exist_ok=True)
        logger.info(f"åˆ†æç»“æœå°†ä¿å­˜åˆ°: {output_path}")
        
        # 1. åŠ è½½æ•°æ®
        logger.info(f"åŠ è½½æ•°æ®æ–‡ä»¶: {csv_file}")
        data = load_csv_data(csv_file)
        logger.info(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(data)} è¡Œï¼Œ{len(data.columns)} åˆ—")
        
        # æ•°æ®æ¦‚è§ˆ
        print("æ•°æ®æ¦‚è§ˆ:")
        print(f"- è¡Œæ•°: {len(data)}")
        print(f"- åˆ—æ•°: {len(data.columns)}")
        print(f"- åˆ—å: {list(data.columns)}")
        
        # 2. ä½¿ç”¨GitAnalyzerè¿›è¡Œå®Œæ•´åˆ†æ
        logger.info("å¼€å§‹åŸºç¡€Gitåˆ†æ...")
        from src.core.analyzer import GitAnalyzer
        
        analyzer = GitAnalyzer()
        
        # æ‰§è¡ŒCSVæ•°æ®åˆ†æ
        results = analyzer.analyze_csv_data(data, project_name)
        
        # æ·»åŠ åŸå§‹æ•°æ®åˆ°ç»“æœä¸­
        results['raw_data'] = data
        
        # 3. DMMé£é™©åˆ†æï¼ˆå•ç‹¬è°ƒç”¨ä»¥ç¡®ä¿ç”Ÿæˆå›¾è¡¨ï¼‰
        logger.info("å¼€å§‹DMMé£é™©åˆ†æ...")
        dmm_results = analyze_dmm_risk(data, logger)
        results['dmm_risk_analysis'] = dmm_results
        
        # 4. æ–‡ä»¶å½±å“åˆ†æ
        possible_rule_files = [
            'steam_notype_all.csv',
            'httpclient_rule_all.csv', 
            'jackrabbit_rule_all.csv',
            'jruby_rule_all.csv',
            'lizard_rule_all.csv'
        ]
        
        rule_file_found = None
        for rule_file in possible_rule_files:
            if os.path.exists(rule_file):
                rule_file_found = rule_file
                break
            # ä¹Ÿæ£€æŸ¥ä¸Šçº§ç›®å½•
            parent_rule_file = os.path.join('..', rule_file)
            if os.path.exists(parent_rule_file):
                rule_file_found = parent_rule_file
                break
        
        if rule_file_found:
            # é€‰æ‹©ä¸€ä¸ªç¤ºä¾‹æ–‡ä»¶è¿›è¡Œå½±å“åˆ†æ
            target_files = [
                'SteamServiceImpl.cs',  # Steamé¡¹ç›®
                'DefaultHttpClient.java',  # HTTPé¡¹ç›®
                'Main.java',  # é€šç”¨
                'App.java'   # é€šç”¨
            ]
            
            for target_file in target_files:
                impact_results = analyze_file_impact(data, rule_file_found, target_file, logger)
                if 'error' not in impact_results and impact_results.get('influenced_files'):
                    results['file_impact_analysis'] = impact_results
                    logger.info(f"æ–‡ä»¶å½±å“é¢„æµ‹å®Œæˆ - ç›®æ ‡æ–‡ä»¶: {target_file}")
                    break
        
        # 5. ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š
        logger.info("ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š...")
        report_path = analyzer.generate_report(results, output_path)
        logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # 6. ç”Ÿæˆé¢å¤–çš„å¯è§†åŒ–å›¾è¡¨ï¼ˆåŸºäºæ‚¨çš„ä»£ç é£æ ¼ï¼‰
        logger.info("ç”Ÿæˆé¢å¤–çš„å¯è§†åŒ–å›¾è¡¨...")
        _generate_additional_charts(data, results, output_path, logger)
        
        # 7. ç”Ÿæˆæ§åˆ¶å°æ‘˜è¦æŠ¥å‘Š
        logger.info("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        print("\n" + "="*50)
        print(f"é¡¹ç›® {project_name} åˆ†æç»“æœæ‘˜è¦")
        print("="*50)
        
        # åŸºç¡€ç»Ÿè®¡
        if 'basic_statistics' in results:
            stats = results['basic_statistics']
            print(f"æäº¤æ€»æ•°: {stats.get('total_commits', 'N/A')}")
            print(f"ä½œè€…æ•°é‡: {stats.get('unique_authors', 'N/A')}")
            print(f"ä¿®æ”¹æ–‡ä»¶æ•°: {stats.get('total_files_changed', 'N/A')}")
        
        # DMMé£é™©åˆ†æ
        if 'dmm_risk_analysis' in results:
            dmm_results = results['dmm_risk_analysis']
            if 'dmm_regression' in dmm_results and 'models' in dmm_results['dmm_regression']:
                best_model = None
                best_r2 = -1
                for model_name, metrics in dmm_results['dmm_regression']['models'].items():
                    if metrics.get('avg_r2', -1) > best_r2:
                        best_r2 = metrics['avg_r2']
                        best_model = model_name
                
                print(f"\nDMMé£é™©é¢„æµ‹:")
                print(f"- æœ€ä½³å›å½’æ¨¡å‹: {best_model}")
                print(f"- RÂ²åˆ†æ•°: {best_r2:.3f}")
        
        # æ–‡ä»¶å½±å“åˆ†æ
        if 'file_impact_analysis' in results:
            impact_results = results['file_impact_analysis']
            target_file = impact_results.get('target_file', 'Unknown')
            influenced_files = impact_results.get('influenced_files', [])
            
            print(f"\næ–‡ä»¶å½±å“åˆ†æ:")
            print(f"- ç›®æ ‡æ–‡ä»¶: {target_file}")
            print(f"- å—å½±å“æ–‡ä»¶æ•°: {len(influenced_files)}")
            
            if influenced_files:
                print(f"- ä¸»è¦å—å½±å“æ–‡ä»¶: {influenced_files[:3]}")
        
        # å…³è”è§„åˆ™åˆ†æ
        if 'association_analysis' in results:
            assoc_results = results['association_analysis']
            if 'analysis' in assoc_results and 'strong_associations' in assoc_results['analysis']:
                strong_rules = assoc_results['analysis']['strong_associations']
                print(f"\nå…³è”è§„åˆ™åˆ†æ:")
                print(f"- å¼ºå…³è”è§„åˆ™æ•°: {len(strong_rules)}")
                if strong_rules:
                    avg_confidence = sum(rule['confidence'] for rule in strong_rules) / len(strong_rules)
                    avg_lift = sum(rule['lift'] for rule in strong_rules) / len(strong_rules)
                    print(f"- å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
                    print(f"- å¹³å‡æå‡åº¦: {avg_lift:.3f}")
        
        # èšç±»åˆ†æ
        if 'clustering_analysis' in results:
            cluster_results = results['clustering_analysis']
            if 'kmeans_clustering' in cluster_results:
                kmeans = cluster_results['kmeans_clustering']
                print(f"\nèšç±»åˆ†æ:")
                print(f"- K-meansæœ€ä¼˜èšç±»æ•°: {kmeans.get('optimal_k', 'N/A')}")
                print(f"- è½®å»“ç³»æ•°: {kmeans.get('best_silhouette_score', 0):.3f}")
        
        print(f"\nåˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        print(f"ğŸ“„ HTMLæŠ¥å‘Š: {report_path}")
        print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: {output_path}/charts/")
        print(f"ğŸ“‹ æ•°æ®æ–‡ä»¶: {output_path}/")
        
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print(f"åˆ†æå¤±è´¥: {str(e)}")


def _generate_additional_charts(data: pd.DataFrame, results: Dict, output_path: Path, logger):
    """
    ç”Ÿæˆé¢å¤–çš„å¯è§†åŒ–å›¾è¡¨ï¼ˆåŸºäºç”¨æˆ·çš„ä»£ç é£æ ¼ï¼‰
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    import networkx as nx
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA
    import numpy as np
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.family'] = 'Songti SC'
    plt.rcParams['axes.unicode_minus'] = False
    
    charts_dir = output_path / "additional_charts"
    charts_dir.mkdir(exist_ok=True)
    
    try:
        # 1. ä½œè€…æ´»è·ƒåº¦åˆ†æï¼ˆç±»ä¼¼æ‚¨çš„ä»£ç é£æ ¼ï¼‰
        if 'author' in data.columns:
            plt.figure(figsize=(12, 8))
            author_counts = data['author'].value_counts().head(15)
            
            plt.subplot(2, 1, 1)
            author_counts.plot(kind='bar', color='skyblue')
            plt.title('ä½œè€…æäº¤æ¬¡æ•°åˆ†å¸ƒ (Top 15)', fontsize=14)
            plt.xlabel('ä½œè€…')
            plt.ylabel('æäº¤æ¬¡æ•°')
            plt.xticks(rotation=45)
            
            # ä½œè€…æäº¤æ—¶é—´åˆ†å¸ƒ
            if 'committer_date' in data.columns:
                plt.subplot(2, 1, 2)
                data['committer_date'] = pd.to_datetime(data['committer_date'], utc=True)
                monthly_commits = data.groupby(data['committer_date'].dt.to_period('M')).size()
                monthly_commits.plot(kind='line', color='green', marker='o')
                plt.title('æœˆåº¦æäº¤è¶‹åŠ¿', fontsize=14)
                plt.xlabel('æ—¶é—´')
                plt.ylabel('æäº¤æ¬¡æ•°')
                plt.xticks(rotation=45)
            
            plt.tight_layout()
            plt.savefig(charts_dir / 'author_activity_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            logger.info("ç”Ÿæˆä½œè€…æ´»è·ƒåº¦åˆ†æå›¾è¡¨")
        
        # 2. æ–‡ä»¶ç±»å‹å’Œå˜æ›´ç±»å‹åˆ†æ
        if 'main_file_type' in data.columns and 'main_change_type' in data.columns:
            plt.figure(figsize=(15, 6))
            
            plt.subplot(1, 2, 1)
            file_type_counts = data['main_file_type'].value_counts()
            plt.pie(file_type_counts.values, labels=file_type_counts.index, autopct='%1.1f%%')
            plt.title('æ–‡ä»¶ç±»å‹åˆ†å¸ƒ')
            
            plt.subplot(1, 2, 2)
            change_type_counts = data['main_change_type'].value_counts()
            plt.pie(change_type_counts.values, labels=change_type_counts.index, autopct='%1.1f%%')
            plt.title('å˜æ›´ç±»å‹åˆ†å¸ƒ')
            
            plt.tight_layout()
            plt.savefig(charts_dir / 'file_and_change_type_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()
            logger.info("ç”Ÿæˆæ–‡ä»¶ç±»å‹å’Œå˜æ›´ç±»å‹åˆ†å¸ƒå›¾è¡¨")
        
        # 3. ä»£ç å˜æ›´è§„æ¨¡åˆ†æ
        if all(col in data.columns for col in ['insertions', 'deletions', 'files']):
            plt.figure(figsize=(15, 10))
            
            # æ’å…¥å’Œåˆ é™¤è¡Œæ•°åˆ†å¸ƒ
            plt.subplot(2, 2, 1)
            plt.scatter(data['insertions'], data['deletions'], alpha=0.6, color='coral')
            plt.xlabel('æ’å…¥è¡Œæ•°')
            plt.ylabel('åˆ é™¤è¡Œæ•°')
            plt.title('æ’å…¥vsåˆ é™¤è¡Œæ•°åˆ†å¸ƒ')
            
            # ä¿®æ”¹æ–‡ä»¶æ•°åˆ†å¸ƒ
            plt.subplot(2, 2, 2)
            plt.hist(data['files'], bins=30, color='lightblue', alpha=0.7)
            plt.xlabel('ä¿®æ”¹æ–‡ä»¶æ•°')
            plt.ylabel('é¢‘æ¬¡')
            plt.title('å•æ¬¡æäº¤ä¿®æ”¹æ–‡ä»¶æ•°åˆ†å¸ƒ')
            
            # ä»£ç è¡Œæ•°å˜åŒ–è¶‹åŠ¿
            if 'committer_date' in data.columns:
                plt.subplot(2, 2, 3)
                data_sorted = data.sort_values('committer_date')
                data_sorted['cumulative_insertions'] = data_sorted['insertions'].cumsum()
                data_sorted['cumulative_deletions'] = data_sorted['deletions'].cumsum()
                
                plt.plot(data_sorted['committer_date'], data_sorted['cumulative_insertions'], 
                        label='ç´¯è®¡æ’å…¥', color='green')
                plt.plot(data_sorted['committer_date'], data_sorted['cumulative_deletions'], 
                        label='ç´¯è®¡åˆ é™¤', color='red')
                plt.xlabel('æ—¶é—´')
                plt.ylabel('ä»£ç è¡Œæ•°')
                plt.title('ç´¯è®¡ä»£ç å˜æ›´è¶‹åŠ¿')
                plt.legend()
                plt.xticks(rotation=45)
            
            # æäº¤è§„æ¨¡åˆ†ç±»
            plt.subplot(2, 2, 4)
            data['commit_size'] = data['insertions'] + data['deletions']
            size_categories = pd.cut(data['commit_size'], 
                                   bins=[0, 10, 50, 200, float('inf')], 
                                   labels=['å°å‹', 'ä¸­å‹', 'å¤§å‹', 'è¶…å¤§å‹'])
            size_counts = size_categories.value_counts()
            plt.bar(size_counts.index, size_counts.values, color=['lightgreen', 'yellow', 'orange', 'red'])
            plt.xlabel('æäº¤è§„æ¨¡')
            plt.ylabel('æäº¤æ¬¡æ•°')
            plt.title('æäº¤è§„æ¨¡åˆ†å¸ƒ')
            
            plt.tight_layout()
            plt.savefig(charts_dir / 'code_change_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            logger.info("ç”Ÿæˆä»£ç å˜æ›´è§„æ¨¡åˆ†æå›¾è¡¨")
        
        # 4. å…³è”è§„åˆ™ç½‘ç»œå›¾ï¼ˆåŸºäºæ‚¨çš„ç½‘ç»œå›¾ä»£ç ï¼‰
        if 'association_analysis' in results and 'association_rules' in results['association_analysis']:
            rules_data = results['association_analysis']['association_rules']
            if rules_data:
                plt.figure(figsize=(16, 16))
                
                # åˆ›å»ºç½‘ç»œå›¾
                G = nx.DiGraph()
                
                for rule in rules_data:
                    antecedents = list(rule['antecedents'])
                    consequents = list(rule['consequents'])
                    lift = rule['lift']
                    
                    for ant in antecedents:
                        for cons in consequents:
                            G.add_edge(ant, cons, weight=lift)
                
                if G.nodes():
                    # è®¡ç®—èŠ‚ç‚¹åº¦æ•°
                    in_degree = dict(G.in_degree())
                    out_degree = dict(G.out_degree())
                    total_degree = {node: in_degree.get(node, 0) + out_degree.get(node, 0)
                                  for node in set(in_degree) | set(out_degree)}
                    
                    # è®¾ç½®å¸ƒå±€
                    pos = nx.spring_layout(G, k=0.3, iterations=50, seed=42)
                    
                    # è·å–è¾¹æƒé‡
                    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
                    edge_colors = [plt.cm.Blues(weight/max(edge_weights)) for weight in edge_weights]
                    
                    # ç»˜åˆ¶ç½‘ç»œå›¾
                    nx.draw(
                        G, pos,
                        with_labels=True,
                        node_size=[total_degree.get(node, 1) * 500 + 1000 for node in G.nodes()],
                        node_color='lightcoral',
                        font_size=8,
                        font_weight="bold",
                        edge_color=edge_colors,
                        width=[w/2 for w in edge_weights],
                        alpha=0.7
                    )
                    
                    plt.title("æ–‡ä»¶å…³è”ç½‘ç»œå›¾ (åŸºäºå…³è”è§„åˆ™)", fontsize=16)
                    plt.axis("off")
                    plt.savefig(charts_dir / 'association_network_enhanced.png', dpi=300, bbox_inches='tight')
                    plt.close()
                    logger.info("ç”Ÿæˆå¢å¼ºç‰ˆå…³è”ç½‘ç»œå›¾")
        
        # 5. DMMæŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾
        dmm_cols = ['dmm_unit_size', 'dmm_unit_complexity', 'dmm_unit_interfacing']
        if all(col in data.columns for col in dmm_cols):
            plt.figure(figsize=(10, 8))
            
            # é€‰æ‹©æ•°å€¼å‹åˆ—è¿›è¡Œç›¸å…³æ€§åˆ†æ
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            correlation_matrix = data[numeric_cols].corr()
            
            # åˆ›å»ºçƒ­åŠ›å›¾
            mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
            sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', 
                       center=0, square=True, linewidths=0.5)
            plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')
            plt.tight_layout()
            plt.savefig(charts_dir / 'correlation_heatmap.png', dpi=300, bbox_inches='tight')
            plt.close()
            logger.info("ç”Ÿæˆç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾")
        
        # 6. é«˜çº§èšç±»åˆ†æï¼ˆåŸºäºæ‚¨çš„UMAPä»£ç é£æ ¼ï¼‰
        _generate_advanced_clustering_analysis(data, charts_dir, logger)
        
        # 7. æ–‡ä»¶ä¿®æ”¹æ¨¡å¼åˆ†æ
        if 'modified_files' in data.columns:
            _generate_file_modification_analysis(data, charts_dir, logger)
        
        logger.info(f"é¢å¤–å›¾è¡¨å·²ä¿å­˜åˆ°: {charts_dir}")
        
    except Exception as e:
        logger.warning(f"ç”Ÿæˆé¢å¤–å›¾è¡¨æ—¶å‡ºç°é”™è¯¯: {str(e)}")


def _generate_advanced_clustering_analysis(data: pd.DataFrame, charts_dir: Path, logger):
    """
    ç”Ÿæˆé«˜çº§èšç±»åˆ†æï¼ˆåŸºäºç”¨æˆ·çš„UMAPä»£ç é£æ ¼ï¼‰
    """
    try:
        import umap
        import matplotlib.pyplot as plt
        from sklearn.preprocessing import StandardScaler, LabelEncoder
        from sklearn.cluster import KMeans
        from sklearn.decomposition import PCA
        import numpy as np
        
        # å‡†å¤‡æ•°æ®è¿›è¡Œèšç±»åˆ†æ
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        
        # ç§»é™¤å¯èƒ½çš„IDåˆ—
        exclude_cols = ['id', 'index', 'Unnamed: 0']
        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        if len(numeric_cols) < 2:
            logger.warning("æ•°å€¼åˆ—ä¸è¶³ï¼Œè·³è¿‡é«˜çº§èšç±»åˆ†æ")
            return
        
        # å¤„ç†åˆ†ç±»å˜é‡
        categorical_cols = ['author', 'main_file_type', 'main_change_type']
        available_cat_cols = [col for col in categorical_cols if col in data.columns]
        
        # åˆ›å»ºç”¨äºèšç±»çš„æ•°æ®é›†
        cluster_data = data[numeric_cols].copy()
        
        # å¤„ç†NaNå€¼ - ä½¿ç”¨å‡å€¼å¡«å……
        logger.info(f"å¤„ç†ç¼ºå¤±å€¼ï¼ŒåŸå§‹æ•°æ®å½¢çŠ¶: {cluster_data.shape}")
        from sklearn.impute import SimpleImputer
        imputer = SimpleImputer(strategy='mean')
        cluster_data_imputed = pd.DataFrame(
            imputer.fit_transform(cluster_data),
            columns=cluster_data.columns,
            index=cluster_data.index
        )
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaNå€¼
        nan_count = cluster_data_imputed.isnull().sum().sum()
        if nan_count > 0:
            logger.warning(f"å¡«å……åä»æœ‰ {nan_count} ä¸ªNaNå€¼ï¼Œä½¿ç”¨0å¡«å……")
            cluster_data_imputed = cluster_data_imputed.fillna(0)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ— ç©·å¤§å€¼
        inf_count = np.isinf(cluster_data_imputed.values).sum()
        if inf_count > 0:
            logger.warning(f"å‘ç° {inf_count} ä¸ªæ— ç©·å¤§å€¼ï¼Œæ›¿æ¢ä¸ºæœ‰é™å€¼")
            cluster_data_imputed = cluster_data_imputed.replace([np.inf, -np.inf], np.nan)
            cluster_data_imputed = cluster_data_imputed.fillna(cluster_data_imputed.mean())
        
        logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {cluster_data_imputed.shape}")
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(cluster_data_imputed)
        
        # æ£€æŸ¥æ ‡å‡†åŒ–åçš„æ•°æ®
        if np.isnan(scaled_data).any() or np.isinf(scaled_data).any():
            logger.error("æ ‡å‡†åŒ–åæ•°æ®ä»åŒ…å«NaNæˆ–æ— ç©·å¤§å€¼")
            return
        
        # åˆ›å»ºå¤§å›¾
        plt.figure(figsize=(20, 15))
        
        # 1. PCAåˆ†æ
        plt.subplot(2, 3, 1)
        pca = PCA(n_components=2)
        pca_result = pca.fit_transform(scaled_data)
        
        # K-meansèšç±»
        kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(scaled_data)
        
        scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], 
                            c=cluster_labels, cmap='tab10', alpha=0.7)
        plt.title(f'PCAèšç±»åˆ†æ (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2f})')
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')
        plt.colorbar(scatter)
        
        # 2. UMAPåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            plt.subplot(2, 3, 2)
            umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
            umap_result = umap_reducer.fit_transform(scaled_data)
            
            scatter = plt.scatter(umap_result[:, 0], umap_result[:, 1], 
                                c=cluster_labels, cmap='tab10', alpha=0.7)
            plt.title('UMAPèšç±»åˆ†æ')
            plt.xlabel('UMAP1')
            plt.ylabel('UMAP2')
            plt.colorbar(scatter)
        except ImportError:
            logger.warning("UMAPåº“æœªå®‰è£…ï¼Œè·³è¿‡UMAPåˆ†æ")
        
        # 3. æŒ‰ä½œè€…åˆ†ç»„çš„èšç±»ï¼ˆå¦‚æœæœ‰ä½œè€…ä¿¡æ¯ï¼‰
        if 'author' in data.columns:
            plt.subplot(2, 3, 3)
            # ç¼–ç ä½œè€…ä¿¡æ¯
            le = LabelEncoder()
            author_encoded = le.fit_transform(data['author'].fillna('Unknown'))
            
            scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], 
                                c=author_encoded, cmap='Set3', alpha=0.7)
            plt.title('æŒ‰ä½œè€…åˆ†ç»„çš„PCAåˆ†å¸ƒ')
            plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')
            plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')
        
        # 4. èšç±»ä¸­å¿ƒåˆ†æ
        plt.subplot(2, 3, 4)
        centers_pca = pca.transform(scaler.inverse_transform(kmeans.cluster_centers_))
        plt.scatter(pca_result[:, 0], pca_result[:, 1], 
                   c=cluster_labels, cmap='tab10', alpha=0.5)
        plt.scatter(centers_pca[:, 0], centers_pca[:, 1], 
                   c='red', marker='x', s=200, linewidths=3, label='èšç±»ä¸­å¿ƒ')
        plt.title('èšç±»ä¸­å¿ƒåˆ†æ')
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')
        plt.legend()
        
        # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
        plt.subplot(2, 3, 5)
        feature_importance = np.abs(pca.components_[0]) + np.abs(pca.components_[1])
        feature_names = numeric_cols
        
        sorted_idx = np.argsort(feature_importance)[-10:]  # å–å‰10ä¸ªé‡è¦ç‰¹å¾
        plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])
        plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])
        plt.title('ç‰¹å¾é‡è¦æ€§ (PCA)')
        plt.xlabel('é‡è¦æ€§åˆ†æ•°')
        
        # 6. èšç±»å¤§å°åˆ†å¸ƒ
        plt.subplot(2, 3, 6)
        cluster_counts = np.bincount(cluster_labels)
        plt.bar(range(len(cluster_counts)), cluster_counts, color='lightblue')
        plt.title('èšç±»å¤§å°åˆ†å¸ƒ')
        plt.xlabel('èšç±»ID')
        plt.ylabel('æ ·æœ¬æ•°é‡')
        
        plt.tight_layout()
        plt.savefig(charts_dir / 'advanced_clustering_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        logger.info("ç”Ÿæˆé«˜çº§èšç±»åˆ†æå›¾è¡¨")
        
    except Exception as e:
        logger.warning(f"ç”Ÿæˆé«˜çº§èšç±»åˆ†ææ—¶å‡ºç°é”™è¯¯: {str(e)}")


def _generate_file_modification_analysis(data: pd.DataFrame, charts_dir: Path, logger):
    """
    ç”Ÿæˆæ–‡ä»¶ä¿®æ”¹æ¨¡å¼åˆ†æ
    """
    try:
        import matplotlib.pyplot as plt
        import ast
        from collections import Counter
        import networkx as nx
        
        # è§£æmodified_filesåˆ—
        all_files = []
        file_pairs = []
        
        for idx, row in data.iterrows():
            try:
                if pd.isna(row['modified_files']):
                    continue
                    
                # å°è¯•è§£ææ–‡ä»¶åˆ—è¡¨
                if isinstance(row['modified_files'], str):
                    if row['modified_files'].startswith('['):
                        files = ast.literal_eval(row['modified_files'])
                    else:
                        files = [f.strip() for f in row['modified_files'].split(',')]
                else:
                    files = [str(row['modified_files'])]
                
                all_files.extend(files)
                
                # ç”Ÿæˆæ–‡ä»¶å¯¹ï¼ˆç”¨äºå…±åŒä¿®æ”¹åˆ†æï¼‰
                for i in range(len(files)):
                    for j in range(i+1, len(files)):
                        file_pairs.append((files[i], files[j]))
                        
            except Exception:
                continue
        
        if not all_files:
            logger.warning("æ— æ³•è§£æmodified_filesæ•°æ®ï¼Œè·³è¿‡æ–‡ä»¶ä¿®æ”¹åˆ†æ")
            return
        
        # åˆ›å»ºåˆ†æå›¾è¡¨
        plt.figure(figsize=(20, 12))
        
        # 1. æœ€å¸¸ä¿®æ”¹çš„æ–‡ä»¶
        plt.subplot(2, 3, 1)
        file_counts = Counter(all_files)
        top_files = file_counts.most_common(15)
        
        if top_files:
            files, counts = zip(*top_files)
            # æˆªæ–­é•¿æ–‡ä»¶å
            short_files = [f[:30] + '...' if len(f) > 30 else f for f in files]
            
            plt.barh(range(len(short_files)), counts, color='skyblue')
            plt.yticks(range(len(short_files)), short_files)
            plt.title('æœ€å¸¸ä¿®æ”¹çš„æ–‡ä»¶ (Top 15)')
            plt.xlabel('ä¿®æ”¹æ¬¡æ•°')
        
        # 2. æ–‡ä»¶æ‰©å±•ååˆ†å¸ƒ
        plt.subplot(2, 3, 2)
        extensions = [f.split('.')[-1] if '.' in f else 'no_ext' for f in all_files]
        ext_counts = Counter(extensions)
        top_exts = ext_counts.most_common(10)
        
        if top_exts:
            exts, counts = zip(*top_exts)
            plt.pie(counts, labels=exts, autopct='%1.1f%%')
            plt.title('æ–‡ä»¶æ‰©å±•ååˆ†å¸ƒ')
        
        # 3. å•æ¬¡æäº¤ä¿®æ”¹æ–‡ä»¶æ•°åˆ†å¸ƒ
        plt.subplot(2, 3, 3)
        files_per_commit = []
        for idx, row in data.iterrows():
            try:
                if pd.isna(row['modified_files']):
                    continue
                if isinstance(row['modified_files'], str):
                    if row['modified_files'].startswith('['):
                        files = ast.literal_eval(row['modified_files'])
                    else:
                        files = [f.strip() for f in row['modified_files'].split(',')]
                    files_per_commit.append(len(files))
            except Exception:
                continue
        
        if files_per_commit:
            plt.hist(files_per_commit, bins=30, color='lightgreen', alpha=0.7)
            plt.title('å•æ¬¡æäº¤ä¿®æ”¹æ–‡ä»¶æ•°åˆ†å¸ƒ')
            plt.xlabel('ä¿®æ”¹æ–‡ä»¶æ•°')
            plt.ylabel('æäº¤æ¬¡æ•°')
        
        # 4. æ–‡ä»¶å…±åŒä¿®æ”¹ç½‘ç»œï¼ˆå‰20ä¸ªæœ€å¸¸è§çš„æ–‡ä»¶å¯¹ï¼‰
        plt.subplot(2, 3, 4)
        if file_pairs:
            pair_counts = Counter(file_pairs)
            top_pairs = pair_counts.most_common(20)
            
            G = nx.Graph()
            for (file1, file2), count in top_pairs:
                # æˆªæ–­æ–‡ä»¶å
                short_file1 = file1.split('/')[-1][:15]
                short_file2 = file2.split('/')[-1][:15]
                G.add_edge(short_file1, short_file2, weight=count)
            
            if G.nodes():
                pos = nx.spring_layout(G, k=0.5, iterations=50)
                
                # ç»˜åˆ¶è¾¹
                edges = G.edges()
                weights = [G[u][v]['weight'] for u, v in edges]
                nx.draw_networkx_edges(G, pos, width=[w/max(weights)*3 for w in weights], 
                                     alpha=0.6, edge_color='gray')
                
                # ç»˜åˆ¶èŠ‚ç‚¹
                nx.draw_networkx_nodes(G, pos, node_color='lightcoral', 
                                     node_size=300, alpha=0.8)
                
                # ç»˜åˆ¶æ ‡ç­¾
                nx.draw_networkx_labels(G, pos, font_size=8)
                
                plt.title('æ–‡ä»¶å…±åŒä¿®æ”¹ç½‘ç»œ')
                plt.axis('off')
        
        # 5. ç›®å½•å±‚çº§åˆ†æ
        plt.subplot(2, 3, 5)
        directories = []
        for f in all_files:
            if '/' in f:
                directories.append('/'.join(f.split('/')[:-1]))
            else:
                directories.append('root')
        
        dir_counts = Counter(directories)
        top_dirs = dir_counts.most_common(10)
        
        if top_dirs:
            dirs, counts = zip(*top_dirs)
            # æˆªæ–­é•¿ç›®å½•å
            short_dirs = [d[:25] + '...' if len(d) > 25 else d for d in dirs]
            
            plt.barh(range(len(short_dirs)), counts, color='orange')
            plt.yticks(range(len(short_dirs)), short_dirs)
            plt.title('æœ€æ´»è·ƒçš„ç›®å½• (Top 10)')
            plt.xlabel('ä¿®æ”¹æ¬¡æ•°')
        
        # 6. æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ¨¡å¼ï¼ˆå¦‚æœæœ‰æ—¶é—´ä¿¡æ¯ï¼‰
        if 'committer_date' in data.columns:
            plt.subplot(2, 3, 6)
            
            # æŒ‰å°æ—¶ç»Ÿè®¡ä¿®æ”¹æ´»åŠ¨
            data['committer_date'] = pd.to_datetime(data['committer_date'], utc=True)
            hourly_activity = data['committer_date'].dt.hour.value_counts().sort_index()
            
            plt.plot(hourly_activity.index, hourly_activity.values, marker='o', color='purple')
            plt.title('æ¯æ—¥ä¿®æ”¹æ´»åŠ¨æ¨¡å¼')
            plt.xlabel('å°æ—¶')
            plt.ylabel('æäº¤æ¬¡æ•°')
            plt.xticks(range(0, 24, 2))
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(charts_dir / 'file_modification_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        logger.info("ç”Ÿæˆæ–‡ä»¶ä¿®æ”¹æ¨¡å¼åˆ†æå›¾è¡¨")
        
    except Exception as e:
        logger.warning(f"ç”Ÿæˆæ–‡ä»¶ä¿®æ”¹åˆ†ææ—¶å‡ºç°é”™è¯¯: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ”¹è¿›çš„CSVæ•°æ®åˆ†æå·¥å…· - æ”¯æŒä»GitHubä»“åº“ç›´æ¥æ”¶é›†æ•°æ®',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åˆ†æç°æœ‰CSVæ–‡ä»¶
  %(prog)s commits_steam_nomsg.csv --project-name steam
  
  # ä»GitHubä»“åº“æ”¶é›†æ•°æ®å¹¶åˆ†æ
  %(prog)s --repo-url https://github.com/BeyondDimension/SteamTools --project-name steamtools
  
  # æŒ‡å®šæ—¶é—´èŒƒå›´æ”¶é›†æ•°æ®
  %(prog)s --repo-url https://github.com/apache/flink --since 2023-01-01 --to 2023-12-31 --project-name flink
        """
    )
    
    # æ•°æ®æºé€‰é¡¹ (äº’æ–¥)
    source_group = parser.add_mutually_exclusive_group(required=True)
    source_group.add_argument('csv_file', nargs='?', help='CSVæ–‡ä»¶è·¯å¾„')
    source_group.add_argument('--repo-url', help='GitHubä»“åº“URL')
    
    # é¡¹ç›®ä¿¡æ¯
    parser.add_argument('--project-name', '-p', required=True, help='é¡¹ç›®åç§°')
    parser.add_argument('--output', '-o', help='è¾“å‡ºç›®å½•')
    
    # ä»“åº“æ”¶é›†é€‰é¡¹
    parser.add_argument('--since', help='å¼€å§‹æ—¶é—´ (æ ¼å¼: YYYY-MM-DD)')
    parser.add_argument('--to', help='ç»“æŸæ—¶é—´ (æ ¼å¼: YYYY-MM-DD)')
    parser.add_argument('--branch', help='æŒ‡å®šåˆ†æçš„åˆ†æ”¯')
    parser.add_argument('--file-types', nargs='+', help='æŒ‡å®šæ–‡ä»¶ç±»å‹ (å¦‚: .py .java .cpp)')
    parser.add_argument('--skip-empty-dmm', action='store_true', default=True,
                       help='è·³è¿‡DMMå€¼ä¸ºç©ºçš„æäº¤ (é»˜è®¤: True)')
    parser.add_argument('--include-empty-dmm', action='store_true',
                       help='åŒ…å«DMMå€¼ä¸ºç©ºçš„æäº¤')
    parser.add_argument('--csv-output', help='æ”¶é›†æ•°æ®æ—¶çš„CSVè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    # æ–­ç‚¹ç»­ä¼ é€‰é¡¹
    parser.add_argument('--resume', action='store_true', default=True,
                       help='å¯ç”¨æ–­ç‚¹ç»­ä¼  (é»˜è®¤: True)')
    parser.add_argument('--no-resume', action='store_true',
                       help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°å¼€å§‹æ”¶é›†')
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument('-v', '--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        import logging
        logging.basicConfig(level=logging.DEBUG)
    
    logger = setup_logger('MainAnalyzer')
    
    try:
        csv_file_to_analyze = None
        
        # å¦‚æœæŒ‡å®šäº†ä»“åº“URLï¼Œå…ˆæ”¶é›†æ•°æ®
        if args.repo_url:
            logger.info("ä»GitHubä»“åº“æ”¶é›†æ•°æ®æ¨¡å¼")
            
            skip_empty_dmm = args.skip_empty_dmm and not args.include_empty_dmm
            
            csv_file_to_analyze = collect_repository_data(
                repo_url=args.repo_url,
                output_file=args.csv_output,
                since=args.since,
                to=args.to,
                branch=args.branch,
                file_types=args.file_types,
                skip_empty_dmm=skip_empty_dmm,
                resume=args.resume and not args.no_resume,
                logger=logger
            )
            
            logger.info(f"æ•°æ®æ”¶é›†å®Œæˆï¼Œå¼€å§‹åˆ†ææ–‡ä»¶: {csv_file_to_analyze}")
            
        else:
            # ä½¿ç”¨ç°æœ‰CSVæ–‡ä»¶
            csv_file_to_analyze = args.csv_file
            
            if not os.path.exists(csv_file_to_analyze):
                print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {csv_file_to_analyze}")
                return 1
        
        # æ‰§è¡Œç»¼åˆåˆ†æ
        comprehensive_analysis(csv_file_to_analyze, args.project_name, args.output)
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main()) 