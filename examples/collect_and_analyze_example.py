#!/usr/bin/env python3
"""
GitAnalytics æ•°æ®æ”¶é›†å’Œåˆ†æç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ï¼š
1. ä»GitHubä»“åº“æ”¶é›†commitæ•°æ®
2. å¯¹æ”¶é›†çš„æ•°æ®è¿›è¡Œç»¼åˆåˆ†æ
3. ç”Ÿæˆåˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
"""

import sys
import os
import logging
from datetime import datetime, timedelta

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from data_collection.repository_collector import RepositoryCollector
from core.analyzer import GitAnalyzer
from analysis.risk_predictor import RiskPredictor
from analysis.file_impact_predictor import FileImpactPredictor
from utils.logger import setup_logger

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´çš„æ•°æ®æ”¶é›†å’Œåˆ†ææµç¨‹"""
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger('ExampleAnalysis')
    logger.info("å¼€å§‹GitAnalyticsç¤ºä¾‹åˆ†æ")
    
    # é…ç½®å‚æ•°
    repo_url = "https://github.com/BeyondDimension/SteamTools"
    project_name = "steamtools_example"
    output_dir = "example_results"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    os.chdir(output_dir)
    
    try:
        # æ­¥éª¤1: æ”¶é›†GitHubä»“åº“æ•°æ®
        logger.info(f"æ­¥éª¤1: ä»GitHubä»“åº“æ”¶é›†æ•°æ® - {repo_url}")
        
        collector = RepositoryCollector()
        
        # æ”¶é›†æœ€è¿‘6ä¸ªæœˆçš„æ•°æ®
        since_date = datetime.now() - timedelta(days=180)
        
        csv_file = collector.collect_repository_data(
            repo_url=repo_url,
            output_file=f"{project_name}_commits.csv",
            since=since_date,
            skip_empty_dmm=True  # åªæ”¶é›†æœ‰DMMå€¼çš„æäº¤
        )
        
        logger.info(f"æ•°æ®æ”¶é›†å®Œæˆï¼Œä¿å­˜åˆ°: {csv_file}")
        
        # æ­¥éª¤2: åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        logger.info("æ­¥éª¤2: åŠ è½½å’Œé¢„å¤„ç†æ•°æ®")
        
        import pandas as pd
        data = pd.read_csv(csv_file)
        logger.info(f"åŠ è½½äº† {len(data)} æ¡commitè®°å½•")
        
        # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
        print("\n" + "="*50)
        print("æ•°æ®æ¦‚è§ˆ")
        print("="*50)
        print(f"æ€»æäº¤æ•°: {len(data)}")
        print(f"æ—¶é—´èŒƒå›´: {data['author_date'].min()} åˆ° {data['author_date'].max()}")
        print(f"ä½œè€…æ•°é‡: {data['author'].nunique()}")
        print(f"ä¸»è¦æ–‡ä»¶ç±»å‹: {data['main_file_type'].value_counts().head()}")
        
        # æ­¥éª¤3: ç»¼åˆåˆ†æ
        logger.info("æ­¥éª¤3: æ‰§è¡Œç»¼åˆåˆ†æ")
        
        config = {
            'enable_quality_analysis': True,
            'enable_association_mining': True,
            'enable_clustering': True,
            'enable_prediction': True,
            'enable_risk_prediction': True,
            'enable_file_impact': False  # éœ€è¦å…³è”è§„åˆ™æ–‡ä»¶
        }
        
        analyzer = GitAnalyzer(config)
        results = analyzer.analyze_csv_data(data, project_name)
        
        # æ­¥éª¤4: DMMé£é™©é¢„æµ‹
        logger.info("æ­¥éª¤4: DMMé£é™©é¢„æµ‹åˆ†æ")
        
        # æ£€æŸ¥DMMå­—æ®µ
        dmm_fields = ['dmm_unit_size', 'dmm_unit_complexity', 'dmm_unit_interfacing']
        if all(field in data.columns for field in dmm_fields):
            risk_predictor = RiskPredictor(logger)
            dmm_results = risk_predictor.predict_dmm_risk(data)
            
            if 'dmm_regression' in dmm_results:
                print("\n" + "="*50)
                print("DMMé£é™©é¢„æµ‹ç»“æœ")
                print("="*50)
                
                regression_results = dmm_results['dmm_regression']
                if 'models' in regression_results:
                    for model_name, metrics in regression_results['models'].items():
                        print(f"{model_name}:")
                        print(f"  - RÂ²åˆ†æ•°: {metrics.get('avg_r2', 0):.3f}")
                        print(f"  - RMSE: {metrics.get('avg_rmse', 0):.3f}")
        
        # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
        logger.info("æ­¥éª¤5: ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        
        print("\n" + "="*50)
        print("åˆ†æå®Œæˆæ€»ç»“")
        print("="*50)
        
        # åŸºç¡€ç»Ÿè®¡
        if 'basic_stats' in results:
            stats = results['basic_stats']
            print(f"âœ“ åŸºç¡€ç»Ÿè®¡åˆ†æå®Œæˆ")
            print(f"  - æ€»æäº¤æ•°: {stats.get('total_commits', 'N/A')}")
            print(f"  - ä½œè€…æ•°é‡: {stats.get('unique_authors', 'N/A')}")
        
        # èšç±»åˆ†æ
        if 'clustering_analysis' in results:
            print(f"âœ“ èšç±»åˆ†æå®Œæˆ")
            cluster_results = results['clustering_analysis']
            if 'kmeans_clustering' in cluster_results:
                kmeans = cluster_results['kmeans_clustering']
                print(f"  - æœ€ä¼˜èšç±»æ•°: {kmeans.get('optimal_k', 'N/A')}")
                print(f"  - è½®å»“ç³»æ•°: {kmeans.get('best_silhouette_score', 0):.3f}")
        
        # å…³è”è§„åˆ™åˆ†æ
        if 'association_analysis' in results:
            print(f"âœ“ å…³è”è§„åˆ™åˆ†æå®Œæˆ")
            assoc_results = results['association_analysis']
            if 'rule_statistics' in assoc_results:
                rule_stats = assoc_results['rule_statistics']
                print(f"  - è§„åˆ™æ€»æ•°: {rule_stats.get('total_rules', 'N/A')}")
                print(f"  - å¹³å‡ç½®ä¿¡åº¦: {rule_stats.get('avg_confidence', 0):.3f}")
        
        # æœºå™¨å­¦ä¹ é¢„æµ‹
        if 'prediction_analysis' in results:
            print(f"âœ“ æœºå™¨å­¦ä¹ é¢„æµ‹å®Œæˆ")
        
        print(f"\nğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°å½“å‰ç›®å½•")
        print(f"ğŸ“ åˆ†æç»“æœä¿å­˜åœ¨: {os.getcwd()}")
        
        logger.info("ç¤ºä¾‹åˆ†æå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        print(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main()) 